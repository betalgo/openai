components:
  schemas:
    ModelResponseProperties:
      type: object
      properties:
        metadata:
          $ref: '#/components/schemas/Metadata'
        top_logprobs:
          maximum: 20
          minimum: 0
          type: integer
          description: "An integer between 0 and 20 specifying the number of most likely tokens to\nreturn at each token position, each with an associated log probability.\n"
          nullable: true
        temperature:
          maximum: 2
          minimum: 0
          type: number
          description: "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.\n"
          default: 1
          nullable: true
          example: 1
        top_p:
          maximum: 1
          minimum: 0
          type: number
          description: "An alternative to sampling with temperature, called nucleus sampling,\nwhere the model considers the results of the tokens with top_p probability\nmass. So 0.1 means only the tokens comprising the top 10% probability mass\nare considered.\n\nWe generally recommend altering this or `temperature` but not both.\n"
          default: 1
          nullable: true
          example: 1
        user:
          type: string
          description: "A stable identifier for your end-users. \nUsed to boost cache hit rates by better bucketing similar requests and  to help OpenAI detect and prevent abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).\n"
          example: user-1234
        service_tier:
          $ref: '#/components/schemas/ServiceTier'