components:
  schemas:
    Batch:
      required:
        - id
        - object
        - endpoint
        - input_file_id
        - completion_window
        - status
        - created_at
      type: object
      properties:
        id:
          type: string
        object:
          enum:
            - batch
          type: string
          description: 'The object type, which is always `batch`.'
          x-stainless-const: true
        endpoint:
          type: string
          description: The OpenAI API endpoint used by the batch.
        model:
          type: string
          description: "Model ID used to process the batch, like `gpt-5-2025-08-07`. OpenAI\noffers a wide range of models with different capabilities, performance\ncharacteristics, and price points. Refer to the [model\nguide](https://platform.openai.com/docs/models) to browse and compare available models.\n"
        errors:
          type: object
          properties:
            object:
              type: string
              description: 'The object type, which is always `list`.'
            data:
              type: array
              items:
                $ref: '#/components/schemas/BatchError'
        input_file_id:
          type: string
          description: The ID of the input file for the batch.
        completion_window:
          type: string
          description: The time frame within which the batch should be processed.
        status:
          enum:
            - validating
            - failed
            - in_progress
            - finalizing
            - completed
            - expired
            - cancelling
            - cancelled
          type: string
          description: The current status of the batch.
        output_file_id:
          type: string
          description: The ID of the file containing the outputs of successfully executed requests.
        error_file_id:
          type: string
          description: The ID of the file containing the outputs of requests with errors.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch was created.
        in_progress_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch started processing.
        expires_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch will expire.
        finalizing_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch started finalizing.
        completed_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch was completed.
        failed_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch failed.
        expired_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch expired.
        cancelling_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch started cancelling.
        cancelled_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch was cancelled.
        request_counts:
          $ref: '#/components/schemas/BatchRequestCounts'
        usage:
          required:
            - input_tokens
            - input_tokens_details
            - output_tokens
            - output_tokens_details
            - total_tokens
          type: object
          properties:
            input_tokens:
              type: integer
              description: The number of input tokens.
            input_tokens_details:
              required:
                - cached_tokens
              type: object
              properties:
                cached_tokens:
                  type: integer
                  description: "The number of tokens that were retrieved from the cache. [More on\nprompt caching](https://platform.openai.com/docs/guides/prompt-caching).\n"
              description: A detailed breakdown of the input tokens.
            output_tokens:
              type: integer
              description: The number of output tokens.
            output_tokens_details:
              required:
                - reasoning_tokens
              type: object
              properties:
                reasoning_tokens:
                  type: integer
                  description: The number of reasoning tokens.
              description: A detailed breakdown of the output tokens.
            total_tokens:
              type: integer
              description: The total number of tokens used.
          description: "Represents token usage details including input tokens, output tokens, a\nbreakdown of output tokens, and the total tokens used. Only populated on\nbatches created after September 7, 2025.\n"
        metadata:
          $ref: '#/components/schemas/Metadata'
      x-oaiMeta:
        name: The batch object
        example: "{\n  \"id\": \"batch_abc123\",\n  \"object\": \"batch\",\n  \"endpoint\": \"/v1/completions\",\n  \"model\": \"gpt-5-2025-08-07\",\n  \"errors\": null,\n  \"input_file_id\": \"file-abc123\",\n  \"completion_window\": \"24h\",\n  \"status\": \"completed\",\n  \"output_file_id\": \"file-cvaTdG\",\n  \"error_file_id\": \"file-HOWS94\",\n  \"created_at\": 1711471533,\n  \"in_progress_at\": 1711471538,\n  \"expires_at\": 1711557933,\n  \"finalizing_at\": 1711493133,\n  \"completed_at\": 1711493163,\n  \"failed_at\": null,\n  \"expired_at\": null,\n  \"cancelling_at\": null,\n  \"cancelled_at\": null,\n  \"request_counts\": {\n    \"total\": 100,\n    \"completed\": 95,\n    \"failed\": 5\n  },\n  \"usage\": {\n    \"input_tokens\": 1500,\n    \"input_tokens_details\": {\n      \"cached_tokens\": 1024\n    },\n    \"output_tokens\": 500,\n    \"output_tokens_details\": {\n      \"reasoning_tokens\": 300\n    },\n    \"total_tokens\": 2000\n  },\n  \"metadata\": {\n    \"customer_id\": \"user_123456789\",\n    \"batch_description\": \"Nightly eval job\",\n  }\n}\n"