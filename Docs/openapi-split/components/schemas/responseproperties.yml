components:
  schemas:
    ResponseProperties:
      type: object
      properties:
        previous_response_id:
          anyOf:
            - type: string
              description: "The unique ID of the previous response to the model. Use this to\ncreate multi-turn conversations. Learn more about\n[conversation state](https://platform.openai.com/docs/guides/conversation-state). Cannot be used in conjunction with `conversation`.\n"
            - nullable: true
        model:
          $ref: '#/components/schemas/ModelIdsResponses'
        reasoning:
          anyOf:
            - $ref: '#/components/schemas/Reasoning'
            - nullable: true
        background:
          anyOf:
            - type: boolean
              description: "Whether to run the model response in the background.\n[Learn more](https://platform.openai.com/docs/guides/background).\n"
              default: false
            - nullable: true
        max_output_tokens:
          anyOf:
            - type: integer
              description: "An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\n"
            - nullable: true
        max_tool_calls:
          anyOf:
            - type: integer
              description: "The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n"
            - nullable: true
        text:
          $ref: '#/components/schemas/ResponseTextParam'
        tools:
          $ref: '#/components/schemas/ToolsArray'
        tool_choice:
          $ref: '#/components/schemas/ToolChoiceParam'
        prompt:
          $ref: '#/components/schemas/Prompt'
        truncation:
          anyOf:
            - enum:
                - auto
                - disabled
              type: string
              description: "The truncation strategy to use for the model response.\n- `auto`: If the input to this Response exceeds\n  the model's context window size, the model will truncate the\n  response to fit the context window by dropping items from the beginning of the conversation.\n- `disabled` (default): If the input size will exceed the context window\n  size for a model, the request will fail with a 400 error.\n"
              default: disabled
            - nullable: true